---
title: "Homework 5"
author: "Yiying Wu"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## 1
Read data
```{r,message=FALSE}
urlfile = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

homicide_data <- read_csv(url(urlfile))|>
                  janitor::clean_names()

```

Prepare data
```{r}
homicide_summary=homicide_data|>
  unite("city_state", city, state, sep = ", ") |>
  group_by(city_state) |>
  summarize(
    Total_Homicides = n(),
    Unsolved_Homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")))
```

Proportion test for Baltimore, MD
```{r}
baltimore_data <- filter(homicide_summary, city_state == "Baltimore, MD")
baltimore_test <- prop.test(baltimore_data$Unsolved_Homicides, baltimore_data$Total_Homicides)
baltimore_test |> broom::tidy()
```

Proportion test for all cities
```{r,message=FALSE,warning=FALSE}
all_cities_test <- homicide_summary %>%
  mutate(
    test_result = map2(Unsolved_Homicides, Total_Homicides, ~prop.test(.x, .y)),
    tidy_result = map(test_result, broom::tidy),  # Specify broom::tidy directly
    city_state = city_state  # Preserve city_state in the final result
  ) %>%
  select(-test_result) %>%  # Remove the test_result column
  unnest(tidy_result)

all_cities_test=all_cities_test|>select(city_state, estimate, conf.low, conf.high)
all_cities_test
```

Create a plot that shows the estimates and CIs for each city
```{r, fig.width = 10}
ggplot(all_cities_test, aes(x=reorder(city_state, estimate), y=estimate)) +
  geom_bar(stat="identity") +
  geom_errorbar(aes(ymin=conf.low, ymax=conf.high), width=.2) +
  coord_flip() +
  xlab("City, State") +
  ylab("Proportion of Unsolved Homicides") +
  ggtitle("Proportion of Unsolved Homicides in Various Cities")
```

## 2
import data
```{r}
data_files = tibble(list.files("./data")) |>
  mutate(file_list = paste(list.files("./data")))
```
Creating functions to read multiple datasets in the list
```{r,message=FALSE}
read_files = function(x) {
  
    data = read_csv(paste0("./data/", x))|>
      mutate(file_names = x)
}

arm_dataset = map_df(data_files$file_list, read_files)

arm_dataset
```
Tidy the dataset
```{r}
clean_arm_dataset =
  arm_dataset |>
  janitor::clean_names() |>
  gather(key = week, value = arm_value, week_1:week_8) |>
  mutate(week = str_remove(week, "week_")) |>
  mutate(subject_ID = as.integer(str_extract(file_names, "[0-9][0-9]"))) |>
  mutate(file_names = ifelse(str_detect(file_names, "con") == TRUE,
                             "Control", "Experiment")) |>
  mutate(across(.cols = c(file_names, week, subject_ID), as.factor)) |>
  relocate(file_names, subject_ID, arm_value)

clean_arm_dataset
```

spaghetti plot showing observations on each subject over time
```{r}
clean_arm_dataset |>
  ggplot(aes(week, arm_value, color=subject_ID)) + 
  geom_point(size = 0.2) + 
  geom_line(aes(group = subject_ID), alpha=0.5) +
  facet_grid(~file_names) +
  labs(x = "Week", y = "Arm Value", 
       title = "Arm Values on Each Subject over 8 Weeks in Two Groups",
       col = "Subject ID") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```
In the experimental group, subjects generally experienced an increase in arm measurements over the 8-week period, although the timing and extent of these changes varied among individuals. Conversely, in the control group, subjects' arm measurements fluctuated over time without any clear trend or significant alterations, in contrast to those in the experimental group. Notably, no individuals in the control group achieved arm measurements exceeding 5, whereas approximately half of the participants in the experimental group surpassed this value during the study.

## 3
Generate 5000 datasets from the model
$$x\sim Normal[\mu,\sigma]$$
```{r}
set.seed(8105) # For reproducibility
n = 30 # Sample size
sigma = 5 # Standard deviation
mu_values = 0:6 # True mean values
alpha = 0.05 # Significance level
num_simulations = 5000 # Number of simulations

simulation_results = map_dfr(mu_values, function(mu) {
  tibble(
    mu = mu,
    simulation = map(1:num_simulations, ~ t.test(rnorm(n, mu, sigma))),
    estimate = map_dbl(simulation, ~ broom::tidy(.x)$estimate),
    p_value = map_dbl(simulation, ~ broom::tidy(.x)$p.value),
    reject_null = p_value < alpha
  )
})
```

Plot: Proportion of times the null is rejected (power of the test)
```{r}
power_plot = simulation_results |>
  group_by(mu) |>
  summarize(power = mean(reject_null)) |>
  ggplot(aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(title = "Power of the Test Across Different True Means", 
       x = "True Mean (mu)", y = "Power")
power_plot
```

